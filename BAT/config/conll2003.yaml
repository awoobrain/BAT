TRAINDATASET:
  DATADIR       : "data"
  FILENAMES     : ["train.txt","dev.txt"]
  MAX_SEQ_LENGTH: 512  # must over 300

TESTDATASET:
  DATADIR       : "data"
  FILENAMES     : ["test.txt"]
  MAX_SEQ_LENGTH: 512  # must over 300

BACKBONE:
  MODEL_NAME: "xlm-roberta-large"  # "bert-base-cased", "xlm-roberta-large"
  EMBED_DIMS: 1024  # embedding size for each token.

LANGUAGE_MODEL:
  NUM_HEADS            : 16  # number of attention heads
  DROPOUT_RATE         : 0.0  # dropout rate. 
  ENCODER_LAYER        : 6  # total encoder layer
  CLASS_NUM            : 9
  ATT_CONV_KERNEL_SIZE : [1,3]
  FFNN_CONV_KERNEL_SIZE: 1

TRAINING:
  SHUFFLE_DATA: true
  SAVE_TIMES  : 10
  EPOCH       : 200
  BATCH_SIZE  : 4  # Batch size.

  LOSS               : "ce_loss"  # "cecla_loss", "pbp_loss", "ce_loss"
  LOSS_GAMMA         : 1.
  LOSS_ALPHA_AMPLIFY : 1.0 

  WEIGHT_LOAD_PATH: ""
  WEIGHT_SAVE_PATH: "weight/conll_weights.json"

  LEARNING_RATE_SELECT     : "bat_lr"
  LEARNING_RATE_WARMUP_STEP: 4000.0 
  LEARNING_RATE_ALPHA      : -0.5
  LEARNING_RATE_BETA       : 0.0
  
  USE_AUTO_GRAPH   : True
  SAVE_INIT_WEIGHTS: True
  VALID_INTERVAL   : 10
  